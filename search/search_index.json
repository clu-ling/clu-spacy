{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is it? \uf0c1 clu-spacy is a library which provides utility methods and API for interfacing spaCy and clu-processors . This allows for the use of spaCy NLP within clu projects. How do I use it? \uf0c1 See our tutorial . Python API documentation is also available.","title":"Home"},{"location":"#what-is-it","text":"clu-spacy is a library which provides utility methods and API for interfacing spaCy and clu-processors . This allows for the use of spaCy NLP within clu projects.","title":"What is it?"},{"location":"#how-do-i-use-it","text":"See our tutorial . Python API documentation is also available.","title":"How do I use it?"},{"location":"contributing/","text":"","title":"Contributing"},{"location":"data/","text":"Training Data \uf0c1 As of spaCy v3.0 the main method in training a custom NLP pipeline is the spacy train command. This method requires data in the spaCy binary format which is serialized with the .spacy extension. There are conversion utilities developed by spaCy for .conllu , .iob , and .json formats. (The .json format is used in spaCy v2.0). The clu-spacy library also provides utility methods for converting data and checking tokenization of the custom data against the spaCy blank lang . Using the command convert-data on custom CoNLL and IOB data will produce match files and offer some retokenization. Run the following command within the data directory: docker ... When the data is not in CoNLL format the user must define the DocBin object manually. This can be accomplished with some preprocessing code, for example (from spaCy documentation , annotated): import spacy # The DocBin oject is used for serializing data for training. from spacy.tokens import DocBin # A blank Language object is a pipeline with no components. nlp = spacy.blank(\"en\") training_data = [ (\"Tokyo Tower is 333m tall.\", [(0, 11, \"BUILDING\")]), ] # the DocBin will store the example documents db = DocBin() for text, annotations in training_data: # Create doc object for text, tokenization defined by base language (\"en\") doc = nlp(text) ents = [] for start, end, label in annotations: span = doc.char_span(start, end, label=label) ents.append(span) doc.ents = ents db.add(doc) db.to_disk(\"./train.spacy\")","title":"Data"},{"location":"data/#training-data","text":"As of spaCy v3.0 the main method in training a custom NLP pipeline is the spacy train command. This method requires data in the spaCy binary format which is serialized with the .spacy extension. There are conversion utilities developed by spaCy for .conllu , .iob , and .json formats. (The .json format is used in spaCy v2.0). The clu-spacy library also provides utility methods for converting data and checking tokenization of the custom data against the spaCy blank lang . Using the command convert-data on custom CoNLL and IOB data will produce match files and offer some retokenization. Run the following command within the data directory: docker ... When the data is not in CoNLL format the user must define the DocBin object manually. This can be accomplished with some preprocessing code, for example (from spaCy documentation , annotated): import spacy # The DocBin oject is used for serializing data for training. from spacy.tokens import DocBin # A blank Language object is a pipeline with no components. nlp = spacy.blank(\"en\") training_data = [ (\"Tokyo Tower is 333m tall.\", [(0, 11, \"BUILDING\")]), ] # the DocBin will store the example documents db = DocBin() for text, annotations in training_data: # Create doc object for text, tokenization defined by base language (\"en\") doc = nlp(text) ents = [] for start, end, label in annotations: span = doc.char_span(start, end, label=label) ents.append(span) doc.ents = ents db.add(doc) db.to_disk(\"./train.spacy\")","title":"Training Data"},{"location":"install/","text":"Installation \uf0c1 clu-spacy can be run one of two ways: Local installation of Python library (>= v3.8) Using Docker Python \uf0c1 Requirements \uf0c1 Python (>= v3.8) Install \uf0c1 To install directly from the default branch of the repository: pip install git+https://github.com/clu-ling/clu-spacy.git Docker \uf0c1 Requirements \uf0c1 Docker Install \uf0c1 Docker images are periodically published to DockerHub : docker pull \"parsertongue/clu-spacy:latest\"","title":"Installation"},{"location":"install/#installation","text":"clu-spacy can be run one of two ways: Local installation of Python library (>= v3.8) Using Docker","title":"Installation"},{"location":"install/#python","text":"","title":"Python"},{"location":"install/#requirements","text":"Python (>= v3.8)","title":"Requirements"},{"location":"install/#install","text":"To install directly from the default branch of the repository: pip install git+https://github.com/clu-ling/clu-spacy.git","title":"Install"},{"location":"install/#docker","text":"","title":"Docker"},{"location":"install/#requirements_1","text":"Docker","title":"Requirements"},{"location":"install/#install_1","text":"Docker images are periodically published to DockerHub : docker pull \"parsertongue/clu-spacy:latest\"","title":"Install"},{"location":"pipefaq/","text":"Training a Pipeline FAQ \uf0c1 What kind of data structure is supported? \uf0c1 In training a pipeline through the spacy train command, data must be in either .conllu or .iob format. These formats are then converted to binary in the .spacy format for training. Can I use custom models? \uf0c1 Yes, spaCy allows for easy integration of custom models in training. You can use custom models in both built-in and custom components. In the config file this would be difined under the components.ner.model block (ner is an example component here), and the python code would be attached via the --code override in the spacy train command.","title":"FAQ"},{"location":"pipefaq/#training-a-pipeline-faq","text":"","title":"Training a Pipeline FAQ"},{"location":"pipefaq/#what-kind-of-data-structure-is-supported","text":"In training a pipeline through the spacy train command, data must be in either .conllu or .iob format. These formats are then converted to binary in the .spacy format for training.","title":"What kind of data structure is supported?"},{"location":"pipefaq/#can-i-use-custom-models","text":"Yes, spaCy allows for easy integration of custom models in training. You can use custom models in both built-in and custom components. In the config file this would be difined under the components.ner.model block (ner is an example component here), and the python code would be attached via the --code override in the spacy train command.","title":"Can I use custom models?"},{"location":"pipeline/","text":"Training \uf0c1 The following methods for training a spaCy pipeline are based on the spaCy documentation . Training from command line \uf0c1 A spaCy pipeline can be trained via the command line using a config.cfg file. The config.cfg can be generated in one of two ways: init method : python -m spacy init config config.cfg [options] manually: You can use spaCy's quickstart and init fill-config After the appropriate config.cfg is generated (see here for an example config), run the following: python -m spacy train config.cfg --output ./output/path [options] If the paths to the train and dev data are not included in the config.cfg, inclue the tags --paths.train ./train --paths.dev ./dev . This will automatically launch a training sequence for the specified number of epochs and settings in the config. The output directory will be populated with a best and last model. Component Options \uf0c1 You can use the included spaCy factory components or add custom components with nlp.add_pipe() . When using the factory components the user can specify which model to use. These models are Thinc models used by spaCy. Custom components can also make use of these models or can include custom coded models. This code can be attached to the trained pipeline when packaged. Built-in components are ( spaCy ): DependencyParser EntityLinker EntityRecognizer Morphologizer SentenceRecognizer SpanCategorizer Tagger TextCategorizer Tok2Vec These components can be trained with custom architecture or using the default architectures by spaCy. When using the default architectures, using the config quickstart will autofill for this option. Config options \uf0c1 For further details on the options available to the config.cfg see the spaCy docs . Training from script \uf0c1 Language.to_disk() after Language.add_pipe() , create_pipe() deprecated. Adding custom components \uf0c1 https://spacy.io/api/language#component @Language.component(\"my_component\") def my_component(doc): # do soemthing to doc return doc The new my_component can now be added to the pipeline via Language.add_pipe(\"my_component\") . (Language.create_pipe() is now deprecated for user use). Saving and loading the pipeline \uf0c1 spacy train \uf0c1 Specify output directory, run spacy package /path/to/pipeline /path/to/output . Language.to_disK() \uf0c1 After modifying Language object, run Language.to_disk(\"/path/to/pipeline\") then spacy package /pipeline /output .","title":"Pipeline"},{"location":"pipeline/#training","text":"The following methods for training a spaCy pipeline are based on the spaCy documentation .","title":"Training"},{"location":"pipeline/#training-from-command-line","text":"A spaCy pipeline can be trained via the command line using a config.cfg file. The config.cfg can be generated in one of two ways: init method : python -m spacy init config config.cfg [options] manually: You can use spaCy's quickstart and init fill-config After the appropriate config.cfg is generated (see here for an example config), run the following: python -m spacy train config.cfg --output ./output/path [options] If the paths to the train and dev data are not included in the config.cfg, inclue the tags --paths.train ./train --paths.dev ./dev . This will automatically launch a training sequence for the specified number of epochs and settings in the config. The output directory will be populated with a best and last model.","title":"Training from command line"},{"location":"pipeline/#component-options","text":"You can use the included spaCy factory components or add custom components with nlp.add_pipe() . When using the factory components the user can specify which model to use. These models are Thinc models used by spaCy. Custom components can also make use of these models or can include custom coded models. This code can be attached to the trained pipeline when packaged. Built-in components are ( spaCy ): DependencyParser EntityLinker EntityRecognizer Morphologizer SentenceRecognizer SpanCategorizer Tagger TextCategorizer Tok2Vec These components can be trained with custom architecture or using the default architectures by spaCy. When using the default architectures, using the config quickstart will autofill for this option.","title":"Component Options"},{"location":"pipeline/#config-options","text":"For further details on the options available to the config.cfg see the spaCy docs .","title":"Config options"},{"location":"pipeline/#training-from-script","text":"Language.to_disk() after Language.add_pipe() , create_pipe() deprecated.","title":"Training from script"},{"location":"pipeline/#adding-custom-components","text":"https://spacy.io/api/language#component @Language.component(\"my_component\") def my_component(doc): # do soemthing to doc return doc The new my_component can now be added to the pipeline via Language.add_pipe(\"my_component\") . (Language.create_pipe() is now deprecated for user use).","title":"Adding custom components"},{"location":"pipeline/#saving-and-loading-the-pipeline","text":"","title":"Saving and loading the pipeline"},{"location":"pipeline/#spacy-train","text":"Specify output directory, run spacy package /path/to/pipeline /path/to/output .","title":"spacy train"},{"location":"pipeline/#languageto_disk","text":"After modifying Language object, run Language.to_disk(\"/path/to/pipeline\") then spacy package /pipeline /output .","title":"Language.to_disK()"},{"location":"tutorial/","text":"Converting SpaCy Doc and Clu Documents \uf0c1 The clu-spacy library can be used as methods in a python script. The following shows how to import the converter and use it: import spacy # Import the converter utilities from clu.spacy.utils import ConverterUtils as converter # Load a SpaCy Doc object nlp = spacy.load('en_core_web_sm') doc = nlp('The jets flew overhead.') # Convert the SpaCy Doc to a Clu Document cludoc = converter.to_clu_doc(doc) # Convert the Clu Document back to a SpaCy Doc spacydoc = converter.to_spacy_doc(cludoc) # This spacydoc should be the same as the original doc assert len(spacydoc) == len(doc) Training a Pipeline \uf0c1 See Training for a tutorial on using the clu-spacy library to train a SpaCy pipeline.","title":"Usage"},{"location":"tutorial/#converting-spacy-doc-and-clu-documents","text":"The clu-spacy library can be used as methods in a python script. The following shows how to import the converter and use it: import spacy # Import the converter utilities from clu.spacy.utils import ConverterUtils as converter # Load a SpaCy Doc object nlp = spacy.load('en_core_web_sm') doc = nlp('The jets flew overhead.') # Convert the SpaCy Doc to a Clu Document cludoc = converter.to_clu_doc(doc) # Convert the Clu Document back to a SpaCy Doc spacydoc = converter.to_spacy_doc(cludoc) # This spacydoc should be the same as the original doc assert len(spacydoc) == len(doc)","title":"Converting SpaCy Doc and Clu Documents"},{"location":"tutorial/#training-a-pipeline","text":"See Training for a tutorial on using the clu-spacy library to train a SpaCy pipeline.","title":"Training a Pipeline"},{"location":"dev/documentation/","text":"Documentation \uf0c1 API documentation \uf0c1 We use pdoc to generate our API documentation. To develop with live reloading, use the following command: LaTeX math in docstrings To use LaTeX-style equations, we recommend using raw strings for docstrings: r\"\"\"My docstring Thanks to the r prefix, we can write math without needing to escape \\: $$\\sum_{i=1}^{\\vert X \\vert} x_{i}$$ \"\"\" Docker \uf0c1 # execute the following command from the project root: docker run --rm -it -v $PWD:/app \\ -p 8001:8001 \\ parsertongue/clu-spacy:latest \\ pdoc --html -c latex_math=True --force --output-dir docs/api --http 0.0.0.0:8001 clu Open your browser to localhost:8001/clu/spacy to see live updates. Anaconda \uf0c1 source activate clu-spacy # execute the following command from the project root: pdoc --html -c latex_math=True --force --output-dir docs/api --http 0.0.0.0:8001 clu Open your browser to localhost:8001/clu/spacy to see live updates. General documentation \uf0c1 We use mkdocs to generate our site documentation from markdown. Markdown source files are located udner the docs directory. Docker \uf0c1 # execute the following command from the project root: docker run --rm -it -v $PWD:/app \\ -p 8000:8000 \\ parsertongue/clu-spacy:latest \\ mkdocs serve -a 0.0.0.0:8000 Open your browser to localhost:8000 to see live updates. Anaconda \uf0c1 To develop the documentation with live reloading, run the following command: source activate clu-spacy # execute the following command from the project root: mkdocs serve -a 0.0.0.0:8000","title":"Documentation"},{"location":"dev/documentation/#documentation","text":"","title":"Documentation"},{"location":"dev/documentation/#api-documentation","text":"We use pdoc to generate our API documentation. To develop with live reloading, use the following command: LaTeX math in docstrings To use LaTeX-style equations, we recommend using raw strings for docstrings: r\"\"\"My docstring Thanks to the r prefix, we can write math without needing to escape \\: $$\\sum_{i=1}^{\\vert X \\vert} x_{i}$$ \"\"\"","title":"API documentation"},{"location":"dev/documentation/#docker","text":"# execute the following command from the project root: docker run --rm -it -v $PWD:/app \\ -p 8001:8001 \\ parsertongue/clu-spacy:latest \\ pdoc --html -c latex_math=True --force --output-dir docs/api --http 0.0.0.0:8001 clu Open your browser to localhost:8001/clu/spacy to see live updates.","title":"Docker"},{"location":"dev/documentation/#anaconda","text":"source activate clu-spacy # execute the following command from the project root: pdoc --html -c latex_math=True --force --output-dir docs/api --http 0.0.0.0:8001 clu Open your browser to localhost:8001/clu/spacy to see live updates.","title":"Anaconda"},{"location":"dev/documentation/#general-documentation","text":"We use mkdocs to generate our site documentation from markdown. Markdown source files are located udner the docs directory.","title":"General documentation"},{"location":"dev/documentation/#docker_1","text":"# execute the following command from the project root: docker run --rm -it -v $PWD:/app \\ -p 8000:8000 \\ parsertongue/clu-spacy:latest \\ mkdocs serve -a 0.0.0.0:8000 Open your browser to localhost:8000 to see live updates.","title":"Docker"},{"location":"dev/documentation/#anaconda_1","text":"To develop the documentation with live reloading, run the following command: source activate clu-spacy # execute the following command from the project root: mkdocs serve -a 0.0.0.0:8000","title":"Anaconda"},{"location":"dev/formatting/","text":"Formatting and style \uf0c1 Code can be auto-formatted using black : Docker \uf0c1 docker run -it -v $PWD:/app \"parsertongue/clu-spacy:latest\" black Anaconda \uf0c1 source activate clu-spacy # execute the following command from the project root: black Typehints \uf0c1 The code makes use of Python type hints. Docker \uf0c1 To perform type checking, run the following command: docker run -it -v $PWD:/app \"parsertongue/clu-spacy:latest\" mypy --ignore-missing-imports --follow-imports=skip --strict-optional . Anaconda \uf0c1 source activate clu-spacy # execute the following command from the project root: mypy --ignore-missing-imports --follow-imports=skip --strict-optional .","title":"Formatting"},{"location":"dev/formatting/#formatting-and-style","text":"Code can be auto-formatted using black :","title":"Formatting and style"},{"location":"dev/formatting/#docker","text":"docker run -it -v $PWD:/app \"parsertongue/clu-spacy:latest\" black","title":"Docker"},{"location":"dev/formatting/#anaconda","text":"source activate clu-spacy # execute the following command from the project root: black","title":"Anaconda"},{"location":"dev/formatting/#typehints","text":"The code makes use of Python type hints.","title":"Typehints"},{"location":"dev/formatting/#docker_1","text":"To perform type checking, run the following command: docker run -it -v $PWD:/app \"parsertongue/clu-spacy:latest\" mypy --ignore-missing-imports --follow-imports=skip --strict-optional .","title":"Docker"},{"location":"dev/formatting/#anaconda_1","text":"source activate clu-spacy # execute the following command from the project root: mypy --ignore-missing-imports --follow-imports=skip --strict-optional .","title":"Anaconda"},{"location":"dev/install/","text":"Installation \uf0c1 Anaconda \uf0c1 clu-spacy is written for Python >= v3.8 . One option to develop is to install all virtual environment (ex. conda , venv , poetry , etc.). Using conda , the library can be installed interactively with a compatible environment using the following commands: conda create --name clu-spacy python=3.8 ipython source activate clu-spacy # execute the following command from the project root: pip install -e \".[dev]\" [dev] will include dependencies for running tests and generating the documentation. Docker \uf0c1 For those familiar with Docker, another option is to use a container with bind mounts as a development environment. Note that the instructions below assume you're developing using a Linux-based environment (they've also been tested on MacOS Catalina). First, you'll need to build the docker image: docker build -f Dockerfile -t \"parsertongue/clu-spacy:latest\" . Launch a container using this image and connect to it: docker run -it -v $PWD:/app \"parsertongue/clu-spacy:latest /bin/bash\" Thanks to the bind mount, changes made to files locally (i.e., outside of the container) will be reflected inside the running container. The parsertongue/clu-spacy includes Jupyter and iPython: ipython from clu.spacy.utils import ConverterUtils converter = ConverterUtils() Removing old docker containers, images, etc. \uf0c1 If you want to save some space on your machine by removing images and containers you're no longer using, see the instructions here . As always, use caution when deleting things.","title":"Install"},{"location":"dev/install/#installation","text":"","title":"Installation"},{"location":"dev/install/#anaconda","text":"clu-spacy is written for Python >= v3.8 . One option to develop is to install all virtual environment (ex. conda , venv , poetry , etc.). Using conda , the library can be installed interactively with a compatible environment using the following commands: conda create --name clu-spacy python=3.8 ipython source activate clu-spacy # execute the following command from the project root: pip install -e \".[dev]\" [dev] will include dependencies for running tests and generating the documentation.","title":"Anaconda"},{"location":"dev/install/#docker","text":"For those familiar with Docker, another option is to use a container with bind mounts as a development environment. Note that the instructions below assume you're developing using a Linux-based environment (they've also been tested on MacOS Catalina). First, you'll need to build the docker image: docker build -f Dockerfile -t \"parsertongue/clu-spacy:latest\" . Launch a container using this image and connect to it: docker run -it -v $PWD:/app \"parsertongue/clu-spacy:latest /bin/bash\" Thanks to the bind mount, changes made to files locally (i.e., outside of the container) will be reflected inside the running container. The parsertongue/clu-spacy includes Jupyter and iPython: ipython from clu.spacy.utils import ConverterUtils converter = ConverterUtils()","title":"Docker"},{"location":"dev/install/#removing-old-docker-containers-images-etc","text":"If you want to save some space on your machine by removing images and containers you're no longer using, see the instructions here . As always, use caution when deleting things.","title":"Removing old docker containers, images, etc."},{"location":"dev/test/","text":"Testing \uf0c1 Tests are written by extending the TestCase class from the unittest module in the Python standard library. All tests can be found in the tests directory. Docker \uf0c1 All tests can be run using the following command: docker run -it -v $PWD:/app \"parsertongue/clu-spacy:latest\" test-all To run just the unit tests (with code coverage), run the following command: docker run -it -v $PWD:/app \"parsertongue/clu-spacy:latest\" green -vvv --run-coverage Anaconda \uf0c1 source activate clu-spacy # execute the following command from the project root: green -vvv . Typehints \uf0c1 The code makes use of Python type hints. Docker \uf0c1 To perform type checking, run the following command: docker run -it -v $PWD:/app \"parsertongue/clu-spacy:latest\" mypy --ignore-missing-imports --follow-imports=skip --strict-optional . Anaconda \uf0c1 source activate clu-spacy # execute the following command from the project root: mypy --ignore-missing-imports --follow-imports=skip --strict-optional .","title":"Testing"},{"location":"dev/test/#testing","text":"Tests are written by extending the TestCase class from the unittest module in the Python standard library. All tests can be found in the tests directory.","title":"Testing"},{"location":"dev/test/#docker","text":"All tests can be run using the following command: docker run -it -v $PWD:/app \"parsertongue/clu-spacy:latest\" test-all To run just the unit tests (with code coverage), run the following command: docker run -it -v $PWD:/app \"parsertongue/clu-spacy:latest\" green -vvv --run-coverage","title":"Docker"},{"location":"dev/test/#anaconda","text":"source activate clu-spacy # execute the following command from the project root: green -vvv .","title":"Anaconda"},{"location":"dev/test/#typehints","text":"The code makes use of Python type hints.","title":"Typehints"},{"location":"dev/test/#docker_1","text":"To perform type checking, run the following command: docker run -it -v $PWD:/app \"parsertongue/clu-spacy:latest\" mypy --ignore-missing-imports --follow-imports=skip --strict-optional .","title":"Docker"},{"location":"dev/test/#anaconda_1","text":"source activate clu-spacy # execute the following command from the project root: mypy --ignore-missing-imports --follow-imports=skip --strict-optional .","title":"Anaconda"}]}